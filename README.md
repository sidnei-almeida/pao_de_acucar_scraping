# ğŸ›’ PÃ£o de Queijo Scraping

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Selenium](https://img.shields.io/badge/selenium-4.0%2B-orange.svg)](https://www.selenium.dev/)
[![Pandas](https://img.shields.io/badge/pandas-latest-blue.svg)](https://pandas.pydata.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109.2-green.svg)](https://fastapi.tiangolo.com/)

## ğŸ“ DescriÃ§Ã£o

O PÃ£o de Queijo Scraping Ã© um web scraper automatizado desenvolvido para coletar dados nutricionais de produtos do site do PÃ£o de AÃ§Ãºcar. O projeto utiliza tÃ©cnicas avanÃ§adas de web scraping com Selenium para navegar de forma eficiente pelo site e extrair informaÃ§Ãµes detalhadas sobre os produtos. AlÃ©m disso, disponibiliza uma API RESTful para consulta dos dados coletados.

## ğŸŒŸ Funcionalidades Principais

- ğŸ” **Coleta Inteligente de URLs**
  - NavegaÃ§Ã£o automÃ¡tica por categorias de produtos
  - Sistema de scrolling dinÃ¢mico para carregamento de mais produtos
  - Coleta de URLs Ãºnicas evitando duplicatas

- ğŸ“Š **ExtraÃ§Ã£o de Dados Nutricionais**
  - InformaÃ§Ãµes nutricionais detalhadas
  - Dados de porÃ§Ãµes e medidas
  - CategorizaÃ§Ã£o dos produtos

- ğŸ’¾ **Armazenamento e ExportaÃ§Ã£o**
  - ExportaÃ§Ã£o em formato CSV e JSON
  - Logs detalhados do processo de scraping
  - Sistema de backup automÃ¡tico

- ğŸŒ **API RESTful**
  - Consulta de dados nutricionais via HTTP
  - Filtros por categoria e nome do produto
  - PaginaÃ§Ã£o de resultados
  - DocumentaÃ§Ã£o interativa com Swagger UI

## ğŸ—ï¸ Arquitetura do Projeto

```
pao_de_queijo_scraping/
â”œâ”€â”€ browser_config.py      # ConfiguraÃ§Ãµes do navegador e Selenium
â”œâ”€â”€ url_collector.py       # MÃ³dulo de coleta de URLs
â”œâ”€â”€ scraper.py            # Core do scraping de dados
â”œâ”€â”€ scraping_log.py       # Sistema de logging
â”œâ”€â”€ main.py               # Ponto de entrada da aplicaÃ§Ã£o
â”œâ”€â”€ api.py               # API RESTful para consulta dos dados
â””â”€â”€ requirements.txt      # DependÃªncias do projeto
```

## ğŸ”§ Requisitos do Sistema

### Requisitos de Hardware
- Processador: 1.6 GHz ou superior
- MemÃ³ria RAM: 4GB mÃ­nimo (8GB recomendado)
- EspaÃ§o em Disco: 500MB livre
- ConexÃ£o com a Internet: 5Mbps ou superior

### Requisitos de Software
- Python 3.8 ou superior
- Google Chrome ou Chromium
- Chromedriver compatÃ­vel com a versÃ£o do Chrome

## ğŸš€ Como Usar

### InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/pao-de-queijo-scraping.git
cd pao-de-queijo-scraping
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

### Executando o Scraper

1. Para coletar URLs:
```bash
python main.py coletar-urls
```

2. Para fazer o scraping dos dados:
```bash
python main.py scraping
```

### Executando a API

1. Inicie o servidor da API:
```bash
python api.py
```

2. Acesse a documentaÃ§Ã£o da API:
- Abra o navegador e acesse: http://localhost:8000/docs

### Endpoints da API

- `GET /`: InformaÃ§Ãµes bÃ¡sicas da API
- `GET /produtos`: Lista produtos com dados nutricionais
  - ParÃ¢metros:
    - `skip`: NÃºmero de registros para pular (paginaÃ§Ã£o)
    - `limit`: NÃºmero mÃ¡ximo de registros
    - `categoria`: Filtrar por categoria
    - `nome`: Filtrar por nome do produto
- `POST /coletar`: Inicia uma nova coleta de dados
  - ParÃ¢metro:
    - `url_categoria`: URL da categoria para coletar

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ“ Logs e Monitoramento

O sistema mantÃ©m logs detalhados em `scraping_log.py`:
- Sucesso/falha na coleta de URLs
- Tempo de execuÃ§Ã£o
- Erros encontrados
- EstatÃ­sticas de coleta

## âš ï¸ LimitaÃ§Ãµes Conhecidas

- Rate limiting do site (mÃ¡ximo de 100 requisiÃ§Ãµes/minuto)
- Produtos sem informaÃ§Ã£o nutricional sÃ£o ignorados
- Algumas categorias podem estar temporariamente indisponÃ­veis

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie sua Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a Branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request


## ğŸ“ Suporte

- Abra uma issue para reportar bugs
- SugestÃµes de melhorias sÃ£o bem-vindas
- Para questÃµes de seguranÃ§a, envie um email para [sidnei.almeida1806@gmail.com]

## ğŸ™ Agradecimentos

- Equipe do Selenium pelo excelente framework
- Comunidade Python pelos pacotes utilizados
- Contribuidores do projeto

---
Desenvolvido com â¤ï¸

# API de Dados Nutricionais - PÃ£o de AÃ§Ãºcar

API para coleta e consulta de dados nutricionais de produtos do PÃ£o de AÃ§Ãºcar.

## Funcionalidades

- Coleta automÃ¡tica de dados nutricionais
- Interface web para configuraÃ§Ã£o da coleta
- Feedback em tempo real do processo de coleta
- Consulta aos dados coletados
- Suporte a mÃºltiplas categorias de produtos

## Tecnologias

- Python 3.11+
- FastAPI
- Socket.IO
- Selenium
- BeautifulSoup4
- Pandas
- Uvicorn

## InstalaÃ§Ã£o Local

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/pao_de_queijo_scraping.git
cd pao_de_queijo_scraping
```

2. Crie um ambiente virtual e ative-o:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Execute a aplicaÃ§Ã£o:
```bash
python api.py
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em `http://localhost:8000`

## Deploy no Render

1. Crie uma conta no [Render](https://render.com)
2. Conecte seu repositÃ³rio GitHub
3. Crie um novo Web Service
4. Configure as seguintes opÃ§Ãµes:
   - Environment: Python
   - Build Command: `pip install -r requirements.txt`
   - Start Command: `uvicorn api:socket_app --host 0.0.0.0 --port $PORT`
   - Python Version: 3.11

## VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto com as seguintes variÃ¡veis:

```env
PORT=8000
```

## LicenÃ§a

MIT
